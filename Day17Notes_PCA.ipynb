{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Fall22Code/blob/main/Day17Notes_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhCsYWfpjvaQ"
      },
      "source": [
        "# Day 17 Notes: Principal Component Analysis & Dimensionality Reduction\n",
        "\n",
        "## Feature Selection\n",
        "- [Sklearn SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSQ_3YOdiXcv"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#load data\n",
        "drive.mount('/content/drive')\n",
        "data = pandas.read_csv('/content/drive/MyDrive/CS167Fall22/Datasets/irisData.csv')\n",
        "predictors = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
        "target = \"species\"\n",
        "\n",
        "train_data, test_data, train_sln, test_sln = \\\n",
        "    train_test_split(data[predictors], data[target], test_size = 0.2, random_state=41)\n",
        "\n",
        "#fit your selector just like you do when training with a classifier/regressor\n",
        "#only do this after splitting into train and test sets - don't let the test\n",
        "#set spoil your predictions\n",
        "selector = SelectKBest(k=2)\n",
        "selector.fit(train_data,train_sln)\n",
        "\n",
        "print('Here are the scores of each feature:')\n",
        "print(selector.scores_)\n",
        "print(predictors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG6MRSGLihFu"
      },
      "source": [
        "#take a look at the training data\n",
        "train_data[0:6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcngkkWi2Co"
      },
      "source": [
        "#transforming the predictor columns of the training set\n",
        "train_transformed = selector.transform(train_data)\n",
        "\n",
        "print(\"Here's what the training predictors look like after the transformation. \\\n",
        "Notice that it's just the last two columns from the original data.\")\n",
        "train_transformed[0:6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tl7TGE3i2rH"
      },
      "source": [
        "#Now we transform the predictor columns in the test set as well.\n",
        "#Notice that we're using the selector that we trained using the training set.\n",
        "#Do not re-fit it to the test data. \n",
        "test_transformed = selector.transform(test_data)\n",
        "\n",
        "#Now we can use our transformed data with a classifier just like always:\n",
        "clf = KNeighborsClassifier()\n",
        "clf.fit(train_transformed,train_sln)\n",
        "predictions = clf.predict(test_transformed)\n",
        "print('Accuracy:',accuracy_score(test_sln,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGGteificGs2"
      },
      "source": [
        "## Feature Extraction\n",
        "- [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK83Ruili5mL"
      },
      "source": [
        "import pandas\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#load data\n",
        "data = pandas.read_csv('/content/drive/MyDrive/CS167Fall22/Datasets/irisData.csv')\n",
        "predictors = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
        "target = \"species\"\n",
        "\n",
        "train_data, test_data, train_sln, test_sln = \\\n",
        "    train_test_split(data[predictors], data[target], test_size = 0.2, random_state=1)\n",
        "\n",
        "#whiten = True is important for uncorrelated\n",
        "#attributes, and is False by default\n",
        "extractor = PCA(n_components=2, whiten=True)\n",
        "#When fitting with PCA, you do not use the target column - this is an unsupervised learning algorithm\n",
        "extractor.fit(train_data)\n",
        "\n",
        "print('this is the variance/importance of each component')\n",
        "print(extractor.explained_variance_ratio_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EGTKffUjDuy"
      },
      "source": [
        "\n",
        "print(\"Here's what the data looks like before being transformed:\")\n",
        "train_data[0:4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icY85lxEjHcH"
      },
      "source": [
        "train_transformed = extractor.transform(train_data)\n",
        "\n",
        "print(\"Here's what the training predictors look like after the transformation.\")\n",
        "train_transformed[0:4]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0spaia9jJeK"
      },
      "source": [
        "#Now we transform the predictor columns in the test set as well.\n",
        "#Notice that we're using the extractor that we trained using the training set.\n",
        "#Do not re-fit it to the test data. \n",
        "test_transformed = extractor.transform(test_data)\n",
        "\n",
        "#Now we can use our transformed data with a classifier just like always:\n",
        "clf = KNeighborsClassifier()\n",
        "clf.fit(train_transformed,train_sln)\n",
        "predictions = clf.predict(test_transformed)\n",
        "print('Accuracy:',accuracy_score(test_sln,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKjg8r7wjNd2"
      },
      "source": [
        "\n",
        "print('Here are the two vectors (in the original space) that define our 2 new axes:')\n",
        "print(extractor.components_[0])\n",
        "print(extractor.components_[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2_JiVwbjPx0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "#visualizing the new axes\n",
        "#PCA gives it back as numpy array\n",
        "tdf = pandas.DataFrame(train_transformed)\n",
        "#next line: probably not the best way\n",
        "tdf['species'] = pandas.Series(list(train_sln)) \n",
        "\n",
        "\n",
        "setosa_series = tdf[ tdf['species'] == 'Iris-setosa' ]\n",
        "virginica_series = tdf[ tdf['species'] == 'Iris-virginica' ]\n",
        "versicolor_series = tdf[ tdf['species'] == 'Iris-versicolor']\n",
        "\n",
        "plt.plot(setosa_series[0],setosa_series[1],'ro',label='setosa')\n",
        "plt.plot(virginica_series[0],virginica_series[1],'bs',label='virginica')\n",
        "plt.plot(versicolor_series[0],versicolor_series[1],'g^',label='versicolor')\n",
        "plt.legend(loc='upper center')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn_nrSlflEdR"
      },
      "source": [
        "# In Class Exercise\n",
        "Tuesday, November 1st\n",
        "\n",
        "1. Input the boston housing dataset (download from Blackboard datasets)\n",
        "2. Run a Support Vector Regressor ( [Here's the sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) ) to get a baseline to compare to.\n",
        "3. Output the R2 score for your baseline SVR\n",
        "4. Use PCa to reduce the dimensions to 2 \n",
        "5. Run a SVR on your transformed data\n",
        "6. Output the R2 score for your dimensionality reduced data. \n",
        "7. Play around... see what you can change (the number of dimensions, gamma, C, etc), to improve your R2 score. Use the sklearn documentation if you're unsure what these will do.\n",
        "8. Be ready to discuss what combination gave you the best R2 score when we come back to the whole class. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hj4S8rCjSLl"
      },
      "source": [
        "#In Class Exercise:\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "# load data\n",
        "housing_data = pandas.read_csv('/content/drive/MyDrive/CS167Fall22/Datasets/HousingData.csv')\n",
        "# clean the data\n",
        "housing_data['CRIM'].fillna(housing_data['CRIM'].mean(),inplace=True)\n",
        "housing_data['ZN'].fillna(housing_data['ZN'].mean(),inplace=True)\n",
        "housing_data['INDUS'].fillna(housing_data['INDUS'].mean(),inplace=True)\n",
        "housing_data['CHAS'].fillna(housing_data['CHAS'].mean(),inplace=True)\n",
        "housing_data['AGE'].fillna(housing_data['AGE'].mean(),inplace=True)\n",
        "housing_data['LSTAT'].fillna(housing_data['LSTAT'].mean(),inplace=True)\n",
        "\n",
        "# Split the data into the training data and testing data\n",
        "target= 'MEDV'\n",
        "predictors = housing_data.columns.drop(target) # use all of the columns except for MEDV\n",
        "train_data, test_data, train_sln, test_sln = train_test_split(housing_data[predictors], housing_data[target], test_size = 0.2, random_state=41)\n",
        "\n",
        "# Normalize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_data)\n",
        "train_data_normalized = scaler.transform(train_data)\n",
        "test_data_normalized = scaler.transform(test_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEKkhzBkkuu9"
      },
      "source": [
        "#Run a Support Vector Regressor on the data to get a baseline \n",
        "\n",
        "\n",
        "#Output the R2 Score for the test data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uRO_dzJkoqx"
      },
      "source": [
        "#Use a Principal Component Analysis to reduce the dimensions to 2\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMldIoe1kW8T"
      },
      "source": [
        "#Run a Support Vector Regressor on your transformed data\n",
        "\n",
        "\n",
        "\n",
        "#Get the R2 score for the test data.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}