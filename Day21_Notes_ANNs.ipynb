{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Fall22Code/blob/main/Day21_Notes_ANNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PSKyhdVBMBP"
      },
      "source": [
        "# Day 21 Code: Artificial Neural Networks\n",
        "\n",
        "We're going to start off by using sklearn MLP to implement a multilayer perceptron, and then we're going to use a deep learning framework, Tensorflow with Keras to build a neural network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlQnxuSxBGwV"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYGVt9K5DcjG"
      },
      "source": [
        "data = pandas.read_csv('/content/drive/MyDrive/CS167Fall22/Datasets/irisData.csv')\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPcbE8vaBLJW"
      },
      "source": [
        "import pandas\n",
        "import numpy\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Split the dataset\n",
        "predictors = data.columns.drop('species')\n",
        "target = \"species\"\n",
        "train_data, test_data, train_sln, test_sln = train_test_split(data[predictors], data[target], test_size = 0.2, random_state=41)\n",
        "\n",
        "#Normalize Data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_data)\n",
        "train_data_norm = scaler.transform(train_data)\n",
        "test_data_norm = scaler.transform(test_data)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOm5Qh1_BzVx"
      },
      "source": [
        "## Build out a Multilayer Perceptron using Scikit-Learn:\n",
        "Here are the links to the documentation: \n",
        "- [sklearn.neural_network.MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
        "- [sklearn.neural_network.MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u63K8A1mBhOS"
      },
      "source": [
        "# Set up MLP\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "mlp = MLPClassifier(random_state=0,hidden_layer_sizes = (100,), max_iter = 800)\n",
        "mlp.fit(train_data_norm,train_sln)\n",
        "predictions = mlp.predict(test_data_norm)\n",
        "\n",
        "print(\"Accuracy: \", metrics.accuracy_score(test_sln,predictions))\n",
        "\n",
        "# Confusion Matrix\n",
        "vals = data[target].unique() ## possible classification values (species)\n",
        "conf_mat = metrics.confusion_matrix(test_sln, predictions, labels=vals)\n",
        "print(pandas.DataFrame(conf_mat, index = \"True \" + vals, columns = \"Pre \" + vals))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7QVdyQfDLqZ"
      },
      "source": [
        "## In-Class Exercise:\n",
        "\n",
        "1. Read in the Boston Housing dataset\n",
        "2. Normalize your data\n",
        "3. Use a [MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) to predict the price of a house 'MEDV'\n",
        "4. Play around with changing the parameters, see what the best R2 score you can get is. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR0BTwTgBucW"
      },
      "source": [
        "# Your code goes here for the In-Class Exercise\n",
        "# 1. Read in the Boston Housing dataset\n",
        "import pandas\n",
        "housing_data = pandas.read_csv('/content/drive/MyDrive/CS167Fall22/Datasets/HousingData.csv') \n",
        "\n",
        "# clean the data\n",
        "housing_data['CRIM'].fillna(housing_data['CRIM'].mean(),inplace=True)\n",
        "housing_data['ZN'].fillna(housing_data['ZN'].mean(),inplace=True)\n",
        "housing_data['INDUS'].fillna(housing_data['INDUS'].mean(),inplace=True)\n",
        "housing_data['CHAS'].fillna(housing_data['CHAS'].mean(),inplace=True)\n",
        "housing_data['AGE'].fillna(housing_data['AGE'].mean(),inplace=True)\n",
        "housing_data['LSTAT'].fillna(housing_data['LSTAT'].mean(),inplace=True)\n",
        "\n",
        "#Split the dataset\n",
        "predictors = housing_data.columns.drop('MEDV')\n",
        "target = \"MEDV\"\n",
        "train_data, test_data, train_sln, test_sln = train_test_split(housing_data[predictors], housing_data[target], test_size = 0.2, random_state=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Normalize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train_data)\n",
        "train_data_normalized = scaler.transform(train_data)\n",
        "test_data_normalized = scaler.transform(test_data)"
      ],
      "metadata": {
        "id": "vK2oOmSWwACn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Use a MLPRegressor to predict the price of a house 'MEDV'\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn import metrics\n",
        "\n",
        "mlp = MLPRegressor(random_state=0)\n",
        "mlp.fit(train_data_normalized,train_sln)\n",
        "predictions = mlp.predict(test_data_normalized)\n",
        "\n",
        "print(\"MLP Regression R2:\", metrics.r2_score(test_sln, predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SfZHnGRwK8N",
        "outputId": "2aae4891-6648-4b00-8cf5-92fb5fd41dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Regression R2: 0.49660446847147843\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Play around with the parameters\n"
      ],
      "metadata": {
        "id": "dx1E-7_1xL6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaB9j0ZcdaDd"
      },
      "source": [
        "# Introducing Deep Learning Frameworks\n",
        "\n",
        "Go ahead and go up to 'Runtime', and select 'change runtime type' from the dropdown list, select 'GPU'. If you complete this step correctly, the following code should say `Found GPU at: /device:GPU:0` or something similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVu_afildpIY"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bA6K3nymJsg"
      },
      "source": [
        "# Iris Dataset with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kogXVGmkmMNj"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "import numpy\n",
        "\n",
        "# we're going to use the iris dataset, but load it from sklearn \n",
        "iris = load_iris()\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "names = iris['target_names']\n",
        "feature_names = iris['feature_names']\n",
        "\n",
        "# One hot encoding\n",
        "enc = OneHotEncoder()\n",
        "Y = enc.fit_transform(y[:, numpy.newaxis]).toarray()\n",
        "\n",
        "# Split the data set into training and testing\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
        "\n",
        "#normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_norm = scaler.transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display a row of data\n",
        "print(X_train_norm[0,:])\n",
        "print(Y_train[0,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFCzV9zNVgs_",
        "outputId": "3b1a24a7-a4f8-4d49-c041-845120d9a19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.37346331 -0.58519388  0.54075378  0.74234434]\n",
            "[0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfvZyvC5ombh"
      },
      "source": [
        "#build our neural network model\n",
        "n_features = X.shape[1] #X generally stands for our predictors\n",
        "n_classes = Y.shape[1] #Y generally stands for our target\n",
        "\n",
        "model = Sequential(name='iris_1')\n",
        "model.add(Dense(2, input_dim=n_features, activation='relu'))\n",
        "model.add(Dense(2, activation='relu'))\n",
        "model.add(Dense(n_classes, activation='softmax'))\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer='sgd', \n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Keras Model Training APIs](https://keras.io/api/models/model_training_apis/)\n"
      ],
      "metadata": {
        "id": "ejpdgUFBE9oq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2RSJESIoH4G"
      },
      "source": [
        "# train the model\n",
        "model.fit(X_train_norm, Y_train, batch_size=15, epochs=5) #add verbose = 0 to make output minimal\n",
        "\n",
        "# cross-validation; make predictions and get error\n",
        "print(\"----\"*30)\n",
        "mse, acc =  model.evaluate(X_test_norm, Y_test)\n",
        "print('Mean Squared Error:',mse)\n",
        "print('Test accuracy:', acc)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW_tE14Sl8Gy"
      },
      "source": [
        "# Boston Housing Dataset with Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import boston_housing\n",
        "\n",
        "# we're going to use the Bosting housing dataset, but load it from keras \n",
        "housing_data = boston_housing.load_data(test_split=0.2) #it comes with it's own test/train split :) \n",
        "(X_train, Y_train), (X_test, Y_test) = housing_data\n"
      ],
      "metadata": {
        "id": "4byOaffnNQ08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_norm = scaler.transform(X_train)\n",
        "X_test_norm = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "zbqCzfmLPl8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### need to execute this to get access to RSquare function\n",
        "!pip install tensorflow_addons"
      ],
      "metadata": {
        "id": "3_Uw2wAJgaN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "from tensorflow_addons.metrics import RSquare\n",
        "\n",
        "#build our model\n",
        "n_features = X_train.shape[1] # get the number of input values for the input layer\n",
        "\n",
        "model = Sequential(name='boston_housing1')   #initialize the model\n",
        "\n",
        "#add some layers. Dense is a fully connected layer\n",
        "model.add(layers.Dense(64, input_dim= n_features, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "#Since we are doing a regressions, we only want one value as an ouput, so our last layer has a Dense layer with 1 neuron.\n",
        "model.add(layers.Dense(1)) #default activation function is \"linear\"\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer='sgd', \n",
        "              metrics= RSquare())\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "O3FLvnWWPv68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "model.fit(X_train_norm, Y_train, epochs=100, batch_size=15, verbose=1)\n",
        "\n",
        "print(\"----\"*30)\n",
        "\n",
        "# cross-validation; make predictions and get error\n",
        "test_mse_score, test_r2_score = model.evaluate(X_test_norm, Y_test)\n",
        "print('MSE:', test_mse_score)\n",
        "print('r2:', test_r2_score)"
      ],
      "metadata": {
        "id": "bWl-8_qjSwbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7WVqBhjfwGW"
      },
      "source": [
        "# In Class Exercise #2\n",
        "What parameters from the models above do you think you can/should change? \n",
        "\n",
        "\n",
        "Try these\n",
        "- Change the number of neruons in each layer.  \n",
        "- Add a layer to the model.\n",
        "- Change the activation function of the model, [here is the documentation](https://keras.io/api/layers/activations/)\n",
        "- Change the optimizer, [here is the documentation](https://keras.io/api/optimizers/) with a list of options\n",
        "- look at the [metrics](https://keras.io/api/metrics/) try adding another metric.\n"
      ]
    }
  ]
}