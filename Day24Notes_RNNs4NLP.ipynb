{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Fall22Code/blob/main/Day24Notes_RNNs4NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTDTrsuy3BVo"
      },
      "source": [
        "# Recurrent Neural Networks for Natural Language Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnQc446427-T",
        "outputId": "f2a36cad-dda7-49f8-d1f8-66b58e6256c8"
      },
      "source": [
        "#imports and things\n",
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    !pip install -q -U tensorflow-addons\n",
        "    !pip install -q -U transformers\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 12.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 10.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 57.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 49.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4TqE64f3R3d"
      },
      "source": [
        "## Char-RNN\n",
        "\n",
        "### Loading and Preparing the Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXP2YlHO3fMZ",
        "outputId": "92b9c280-23de-4565-ecc1-aca7d1590c44"
      },
      "source": [
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7oFxQlG3lhD",
        "outputId": "56e904df-9e5f-4e14-c9d7-91fc7b20d066"
      },
      "source": [
        "print(shakespeare_text[:148])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "L_QwH8Ev3oOv",
        "outputId": "beb3bf52-b2a7-4b4f-983e-312cb9f6b1d8"
      },
      "source": [
        "# The vocabulary of our character-level language model looks like this:\n",
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWWRdLdg3v-G"
      },
      "source": [
        "# Use Tokenizer to tokenize the Shakespeare text\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAkOhr8236Rf",
        "outputId": "312c2eee-d41c-4f67-e683-878807e74ec6"
      },
      "source": [
        "# Embed the word 'First' as tokens:\n",
        "tokenizer.texts_to_sequences([\"First\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLm-SCCM3-uz",
        "outputId": "45cc1cfd-b16c-45e6-b6e2-1671a99ec43d"
      },
      "source": [
        "# Revert the sequence of tokens back to the word:\n",
        "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqWg3zk94LAY",
        "outputId": "a2c2d88f-9ff8-4b6a-d598-bee1d3b5961e"
      },
      "source": [
        "# Dataset prep\n",
        "max_id = len(tokenizer.word_index) # number of distinct characters\n",
        "dataset_size = tokenizer.document_count # total number of characters\n",
        "\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
        "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
        "\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "\n",
        "dataset = dataset.prefetch(1)\n",
        "\n",
        "\n",
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch.shape, Y_batch.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 100, 39) (32, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HneQNZKY4uWZ"
      },
      "source": [
        "## Creating and Training the Model\n",
        "If you are not connected to a GPU/TPU, this code will likely take hours to run.\n",
        "\n",
        "If you are connected to a GPU/TPU, you should be able to run this at about 5-10 minute per epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvQ1H6TT4O1E",
        "outputId": "aeaf460d-a358-4f07-c29a-4d53bd5aa094"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(64, return_sequences=True, input_shape=[None, max_id],\n",
        "                     dropout=0.2),\n",
        "    keras.layers.GRU(64, return_sequences=True,\n",
        "                     dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
        "                    epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "31370/31370 [==============================] - 405s 13ms/step - loss: 1.8363\n",
            "Epoch 2/5\n",
            "31370/31370 [==============================] - 389s 12ms/step - loss: 1.7544\n",
            "Epoch 3/5\n",
            "31370/31370 [==============================] - 387s 12ms/step - loss: 1.7373\n",
            "Epoch 4/5\n",
            "31370/31370 [==============================] - 385s 12ms/step - loss: 1.7289\n",
            "Epoch 5/5\n",
            "31370/31370 [==============================] - 383s 12ms/step - loss: 1.7238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-1R3kOy5euX"
      },
      "source": [
        "## Using the Model to Generate Text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u3BHV5ak4SRK",
        "outputId": "5ff13217-a05a-4165-9285-eca0a72c4cd5"
      },
      "source": [
        "def preprocess(texts):\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    return tf.one_hot(X, max_id)\n",
        "\n",
        "# Let's pass in 'How are yo' and see what it predicts the next letter should be:\n",
        "X_new = preprocess([\"How are yo\"])\n",
        "\n",
        "#this line takes a look at the softmax output and returns the max\n",
        "Y_pred = np.argmax(model(X_new), axis=-1)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZMtUeBh6JDN"
      },
      "source": [
        "def next_char(text, temperature=1):\n",
        "    X_new = preprocess([text])\n",
        "    y_proba = model(X_new)[0, -1:, :]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DfPS9_936K-C",
        "outputId": "546ae8b6-9cab-402a-c4ab-ed0f224bb3fc"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "next_char(\"How are yo\", temperature=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vDjXtnV6Pgr"
      },
      "source": [
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGQLDBQXH6M_"
      },
      "source": [
        "**Temperature** controls the randomness of the outputs, a larger temperature means a less confident, but more random output (more errors, less logic), while a lower temperature is a more confident but less random output. Take a look below to see how temperature influences the predictions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpjzl6LJ6Umm",
        "outputId": "64f72d31-e004-41bd-e88b-753319ca2e7a"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "print(complete_text(\"t\", temperature=0.3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the words, the counter to the parts,\n",
            "and the not th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XswPMxn_6Xgk",
        "outputId": "ed841a05-0a91-4380-aafb-5701b6e9fe91"
      },
      "source": [
        "print(complete_text(\"t\", temperature=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to co bardon'd to your to\n",
            "shop he ware their resent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heAuNZgb6Zgh",
        "outputId": "f0d3215c-0332-40b0-d164-314f1f67505c"
      },
      "source": [
        "print(complete_text(\"t\", temperature=2))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tpeniomem fvce? t?gmore: yet 'vile deaicurs:\n",
            "-yaeem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phmVSDnsIlSK"
      },
      "source": [
        "# In Class Exercise: \n",
        "\n",
        "With your group, answer the following:\n",
        "- Play around with the `complete_text` function, try different character lengths. What is the best output you got? \n",
        "\n",
        "- Do you think we trained the model long enough? Do you expect the predictions to be better if we made the model larger or trained the model longer? Why or why not?\n",
        "\n",
        "- Does anything surprise you about the predictions? Why or why not?\n",
        "\n",
        "- How would you go about improving the model? What hyperparameters would you consider changing?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pywar4mmHqkZ",
        "outputId": "de1cb303-ff8f-49ad-b2a9-acad0699f791"
      },
      "source": [
        "print(complete_text(\"she\", temperature=0.5, n_chars=1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "she the streng to me have heard the care to be the tents to change her in\n",
            "a patring and in the ching to the care me the will for you are the matter,\n",
            "the reat noble bell bianca, and the heads,\n",
            "the since the construmand the petite to the good farst as the confeit to be as you wood his tongue.\n",
            "\n",
            "first citizen:\n",
            "the tell me the strong, the great and load you to hear of the father ball as my tell and the soot\n",
            "for and sen the counted that come her profituse and counders and it well\n",
            "for the needs the contramuse and the chains,\n",
            "i have the porting with the matter of your words of it the words\n",
            "that you are like the streng his to the comple and from thy country the worth of son stead\n",
            "the subtening as the good the wore, the good montent of men.\n",
            "\n",
            "first citizen:\n",
            "men, the enough to the chain with the ran of the commord:\n",
            "i well the words, you do he\n",
            "is see in the did stand you shall not so say they will cut the love to the report as your comes,\n",
            "the matter to a the pleasure me,\n",
            "the lattle that i have the fat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6sXR1n_Id2g"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}