{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urness/CS167Fall22Code/blob/main/Day24Notes_RNNs4NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTDTrsuy3BVo"
      },
      "source": [
        "# Recurrent Neural Networks for Natural Language Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnQc446427-T"
      },
      "source": [
        "#imports and things\n",
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    !pip install -q -U tensorflow-addons\n",
        "    !pip install -q -U transformers\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4TqE64f3R3d"
      },
      "source": [
        "## Char-RNN\n",
        "\n",
        "### Loading and Preparing the Dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXP2YlHO3fMZ"
      },
      "source": [
        "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "    shakespeare_text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7oFxQlG3lhD"
      },
      "source": [
        "print(shakespeare_text[:148])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_QwH8Ev3oOv"
      },
      "source": [
        "# The vocabulary of our character-level language model looks like this:\n",
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWWRdLdg3v-G"
      },
      "source": [
        "# Use Tokenizer to tokenize the Shakespeare text\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAkOhr8236Rf"
      },
      "source": [
        "# Embed the word 'First' as tokens:\n",
        "tokenizer.texts_to_sequences([\"First\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLm-SCCM3-uz"
      },
      "source": [
        "# Revert the sequence of tokens back to the word:\n",
        "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqWg3zk94LAY",
        "outputId": "697681ee-e37d-4dd1-d09a-6919f39fb0a0"
      },
      "source": [
        "# Dataset prep\n",
        "max_id = len(tokenizer.word_index) # number of distinct characters\n",
        "dataset_size = tokenizer.document_count # total number of characters\n",
        "\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
        "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)\n",
        "\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "\n",
        "dataset = dataset.prefetch(1)\n",
        "\n",
        "\n",
        "for X_batch, Y_batch in dataset.take(1):\n",
        "    print(X_batch.shape, Y_batch.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 100, 39) (32, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HneQNZKY4uWZ"
      },
      "source": [
        "## Creating and Training the Model\n",
        "If you are not connected to a GPU, this code will likely take hours to run.\n",
        "\n",
        "If you are connected to a GPU, you should be able to run this at about 5-10 minute per epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvQ1H6TT4O1E"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(64, return_sequences=True, input_shape=[None, max_id],\n",
        "                     dropout=0.2),\n",
        "    keras.layers.GRU(64, return_sequences=True,\n",
        "                     dropout=0.2),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
        "                                                    activation=\"softmax\"))\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n",
        "                    epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-1R3kOy5euX"
      },
      "source": [
        "## Using the Model to Generate Text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3BHV5ak4SRK"
      },
      "source": [
        "def preprocess(texts):\n",
        "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "    return tf.one_hot(X, max_id)\n",
        "\n",
        "# Let's pass in 'How are yo' and see what it predicts the next letter should be:\n",
        "X_new = preprocess([\"How are yo\"])\n",
        "\n",
        "#this line takes a look at the softmax output and returns the max\n",
        "Y_pred = np.argmax(model(X_new), axis=-1)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZMtUeBh6JDN"
      },
      "source": [
        "def next_char(text, temperature=1):\n",
        "    X_new = preprocess([text])\n",
        "    y_proba = model(X_new)[0, -1:, :]\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfPS9_936K-C"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "next_char(\"How are yo\", temperature=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vDjXtnV6Pgr"
      },
      "source": [
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGQLDBQXH6M_"
      },
      "source": [
        "**Temperature** controls the randomness of the outputs, a larger temperature means a less confident, but more random output (more errors, less logic), while a lower temperature is a more confident but less random output. Take a look below to see how temperature influences the predictions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpjzl6LJ6Umm"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "print(complete_text(\"t\", temperature=0.3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XswPMxn_6Xgk"
      },
      "source": [
        "print(complete_text(\"t\", temperature=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heAuNZgb6Zgh"
      },
      "source": [
        "print(complete_text(\"t\", temperature=2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phmVSDnsIlSK"
      },
      "source": [
        "# In Class Exercise: \n",
        "\n",
        "With your group, answer the following:\n",
        "- Play around with the `complete_text` function, try different character lengths. What is the best output you got? \n",
        "\n",
        "- Do you think we trained the model long enough? Do you expect the predictions to be better if we made the model larger or trained the model longer? Why or why not?\n",
        "\n",
        "- Does anything surprise you about the predictions? Why or why not?\n",
        "\n",
        "- How would you go about improving the model? What hyperparameters would you consider changing?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pywar4mmHqkZ"
      },
      "source": [
        "print(complete_text(\"she\", temperature=0.5, n_chars=1000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6sXR1n_Id2g"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}